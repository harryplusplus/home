---
title: "Withsy"
pubDate: 2025-05-19
description: "Withsy is a user-centric AI chat application that empowers you to tailor your conversational experience. It provides robust features for managing prompts, customizing interactions, and securely saving your valuable chats and messages."
---

import Mermaid from "@/components/Mermaid.astro";

# {frontmatter.title}

## Table of Contents

## What Is Withsy?

{frontmatter.description}

Try it through the [link](https://withsy.chat/).

## Why We Created This?

Withsy는 저와 [Jenn](https://www.hellojennpark.com/) 두 명의 소프트웨어 엔지니어가 만들었습니다.
범람하는 AI 챗앱 시대지만 우리의 사용사례를 위한 AI 챗앱이 없었습니다.
예를 들어, 질문에 대한 Google Gemini의 대답이 마음에 들지 않아서 xAI Grok에게 물어보고 싶을 경우, 대화 뿐만 아니라 대화의 컨텍스트를 전달해야하는데 이 과정은 불편하고 지루합니다.
그래서 하나의 챗앱에서 여러 AI에게 대화의 컨텍스트를 전달하거나 대화를 분기할 수 있는 기능을 만들었습니다.
그리고 특정 질문이나 대답을 추후 확인하거나 재확인하기 위해 즐겨찾기하고 해당 컨텍스트로 바로 이동할 수 있는 기능이 없었습니다.
그래서 Withsy를 만들었습니다.

## Architecture

<Mermaid chart={`
architecture-beta

group withsy(withsy:logo)[Withsy]

service db(aws:arch-amazon-rds-64)[Amazon RDS for Postgres] in withsy
service storage(supabase:supabase-logo-icon)[Supabase Storage] in withsy
service server(google-cloud:cloud-run)[Google Cloud Run] in withsy

server:R -- L:db
server:B -- T:storage
`}/>

### Why Use Google Cloud Run?

저희가 제공하는 서비스는 AI 채팅 웹앱입니다.
일반적인 웹 및 REST API뿐만 아니라 SSE를 사용한 AI 응답 스트리밍을 제공합니다.
그리고 Third party AI API 요청을 백그라운드 작업으로 실행합니다.

게다가 저는 프로덕션 환경과 개발 환경의 유사성을 중요하게 생각합니다.
그래서 도커 컨테이너 환경을 선호합니다.

최근 서버리스 플랫폼들은 Node.js 런타임이 아닌 자체적인 JS 런타임을 사용합니다.
이런 플랫폼들은 작은 팀에서 사용하는 것보다는 대규모 서비스의 최적화를 위해 사용하는 편이 좋다고 생각합니다.

왜냐하면 도커 컨테이너 기반의 플랫폼보다 이런 서버리스 플랫폼이 프로덕션 환경과 개발 환경의 차이가 더 크기 때문입니다.
서버리스 플랫폼은 요청별 요청 수에 따른 분리와 컴퓨팅 자원의 최적화로 비용을 절약할 수 있고 런타임까지 클라우드 플랫폼에서 제공해서 안정성이 높기 때문입니다.

하지만 작은 팀의 신규 프로젝트에서 이런 조기 최적화를 진행할 경우 불필요한 제약사항이 생겨 이 제약사항을 지키기 위해서 기술적으로 시간 자원을 더 투자해야하므로 비효율적이라고 생각합니다.

그리고 무중단 배포 및 자동 스케일 아웃이 가능한 관리형 플랫폼을 우선적으로 고려했습니다.
작은 팀이었기 때문에 쿠버네티스로 인한 시간적 정신적 자원 소모를 하고싶지 않았기 때문입니다.

이런 요구사항을 충족하는 클라우드 플랫폼이 필요했고 Google Cloud Run은 이런 요구사항을 충족했습니다.
무중단 배포, 자동 스케일 아웃, 도커 컨테이너 기반의 관리형 플랫폼입니다.
특히 [요청 타임아웃](https://cloud.google.com/run/docs/configuring/request-timeout#timeout-period)이 최대 3600초입니다.
반면에 [AWS App Runner의 요청 타임아웃](https://docs.aws.amazon.com/apprunner/latest/dg/develop.html)은 120초입니다.

#### Why Use Monolith Server?

먼저 우리는 이 웹앱을 구축하기 위해서 Next.js 프레임워크를 선택했습니다.
팀원 Jenn이 현업에서 다년간 Next.js를 사용했고, 그 동안 좋은 결과물을 만들었기 때문입니다.
그리고 저는 Next.js가 프로덕션 레디 프레임워크라고 생각했습니다. 다양한 회사에서 사용하는 것을 봤기 때문입니다.

하지만 저는 요즘 Node.js 웹앱 개발 아키텍처인 프론트엔드를 위한 백엔드와 백엔드를 위한 백엔드 아키텍처를 피하고 싶었습니다.
왜냐하면 작은 팀의 신규 프로젝트에서 시작부터 2개의 서비스를 개발하고 운영하는 것은 비용이기 때문입니다.
프로덕션에서 두 개의 서비스를 배포해야 합니다.
이 경우 최대 약 두 배의 요금으로 이어질 수 도 있습니다.

그리고 프론트엔드와 백엔드 간 인터페이스 패키지를 유지관리해야하며 자연스럽게 모노리포도 도입해야 합니다.
물론 필요할 경우 사용하면 좋은 기술이지만 현재 저는 불필요하다고 판단했습니다.

신규 기능 추가시 프론트엔드보다 백엔드를 먼저 배포해줘야 하는 배포 순서 의존성도 생겨버립니다.
프론트엔드가 먼저 배포될 경우 사용자는 아직 존재하지 않는 REST API를 요청할 수 있기 때문입니다.

게다가 백그라운드 작업 처리도 같은 서버에서 처리합니다.
이유는 프론트엔드와 백엔드 서비스를 분리하지 않은 것과 같습니다.
서비스를 다른 서버로 분리할 경우 인터페이스 유지 관리 및 배포 순서 의존성이 생기기 때문입니다.

결과적으로 프론트엔드, 백엔드 그리고 백그라운드 작업을 하나의 리포지토리, 하나의 서버에서 처리하도록 했습니다.
개발 스택과 인프라 구조를 가볍게 만들어 실제 비지니스 로직에 집중할 수 있도록 했습니다.

하지만 프론트엔드와 백엔드를 하나의 서버에서 동작시킨 것이 아쉬운 부분도 있었습니다.
개발 중에는 발생하지 않았지만 번들링으로 인해 프로덕션에서 Zod 객체가 순환 참조되면서 런타임 오류를 발생시켰습니다.
번들러로 인해 Minification이 적용됐고, Next.js 15 이상에서 Minification을 끄는 옵션이 없어졌습니다. ([link](https://nextjs.org/docs/architecture/nextjs-compiler#minification))
Minification된 오류 콜스택으로 원인 파악에 어려움이 있었고 코드를 하나씩 되돌리며 이슈를 해결했습니다.

나중에 Next.js 소스 코드를 확인해보니 `next.config.ts`의 `experimental.serverMinification` 옵션을 사용해서 `webpack`의 `optimization.minimize`을 제어함을 확인했습니다.
하지만 이 내용은 Next.js 문서에는 있지 않았고, 고수준의 설정만 노출하고 세부적인 제어를 감추는 방식이어서 설정을 찾는 방식이 조금 아쉬웠습니다.

### Why Use Amazon RDS for Postgres?

Google Cloud Run을 사용한다면 Google Cloud SQL for Postgres을 사용하는 것이 좋다고 생각합니다.
보안상 DB 접근을 Google Cloud 내부에서만 가능하도록 제한할 수 있기 때문입니다.
보안상의 이유에도 불구하고 Amazon RDS for Postgres를 선택한 이유는 AWS Credit이 있었기 때문입니다.
무료 서비스로 배포했기 때문에 갖고 있는 AWS Credit을 최대한 활용하고 싶었습니다.
만약 유료 서비스로 배포한다면 보안상의 이점이 있는 Google Cloud SQL for Postgres를 사용할 것 같습니다.

#### Why Use Postgres?

Postgres는 다재다능한 데이터베이스입니다.
기본적으로 관계형 데이터베이스의 특성을 따릅니다.
하지만 비정형 데이터인 JSON/JSONB 타입을 활용해 NoSQL로 구성할 수 도 있습니다.
그리고 Event Triggers를 사용해 이벤트 기반 아키텍처를 구축할 수 도 있습니다.
특히 메시지 큐도 구현할 수 있고, 이미 Postgres를 사용하는 메시지 큐 구현들도 많습니다.

Withsy에서는 사용자 데이터와 같은 정형 데이터는 테이블을 사용해 구현하고, 사용자의 커스텀 Preferences는 JSONB 타입을 사용해 저장했습니다.
그리고 AI 채팅 요청은 Postgres를 사용한 메시지 큐 구현인 Graphile Workers를 사용해서 API 요청시 작업을 큐에 넣고 API 응답을 즉시 반환하도록 했습니다.
AI API는 요청에 따라서 시간이 오래걸릴 수 도 있고, 실패시 내부적으로 재시도할 수 도 있기 때문입니다.

이처럼 Postgres를 사용하면 특수한 목적의 DB를 사용하지 않을 수도 있습니다.
물론 성능이나 효율성에 따라 특수한 목적의 DB가 필요할 수 도 있습니다.
하지만 작은 팀의 초기 프로젝트에서 Postgres로 많은 기능을 처리하면, 인프라 관리의 복잡성이 낮아지기 때문에 Postgres로 최대한 가능한지 파악하는 것이 좋다고 생각합니다.

### Why Use Supabase Storage?

AI 모델마다 프로필 이미지를 저장할 수 있는 기능을 추가할 때, 파일 저장소 기능이 필요했습니다.
이미지 파일을 저장할 때 Postgres에 저장하는 것보다는 S3와 같은 Storage 서비스를 사용하는 것이 가격이 저렴합니다.
그리고 Google Cloud Storage보다 AWS S3 또는 Supabase Storage가 더 저렴합니다.
Supabase Storage는 AWS S3 API/SDK를 직접 사용할 수 있고 무료 티어가 있어서 Supabase Storage를 선택했습니다.

## AI Chat Response Streaming

### Design

AI 채팅시 브라우저에서 사용자가 메시지 전송 버튼을 클릭하면 서버를 통해 제 3자 AI API를 호출합니다.
이 때

### Sequence Diagram

<Mermaid chart={`
sequenceDiagram

actor us as User

box Server
participant ap as API
participant wo as Worker
end

box Postgres
participant db as DB
participant q as Queue in DB
participant ev as Events in DB
end

participant ai as AI

rect rgba(0, 0, 255, .05)
Note right of us: Send message to AI

us->>+ap: Send message to AI
ap->>+db: Create message
db-->>-ap: message_id
ap->>q: Enqueue "send to ai" task
ap-->>-us: message_id
end

rect rgba(0, 0, 255, .05)
Note right of us: Process AI response

loop Listen or pull task

wo->>+q: Dequeue task
q-->>-wo: task
wo->>+db: Get history
db-->>-wo: history
wo->>ai: Create response stream

loop Until response is done
ai-->>wo: Send response chunk
wo->>db: Create message chunk
db-->>wo: message_chunk_id
wo->>ev: Notify message chunk event
end

end

end

rect rgba(0, 0, 255, .05)
Note right of us: Receive ai response

us->>ap: Receive ai response
ap->>db: Get message chunks
db-->>ap: message chunks

ap->>ev: Listen message chunk event

loop Saved message chunk is done
ap-->>us: message chunk
end

loop Message chunk event is done
ev-->>ap: message chunk event
ap-->>us: message chunk
end

end
`}/>
